# üöÄ Configurar Ollama R√°pido (Gratis)

## Tu Problema Actual

- ‚ùå Cuota de OpenAI excedida
- ‚ùå El sistema necesita una IA funcionando

## Soluci√≥n: Ollama (100% Gratis)

### Paso 1: Instalar Ollama en tu Ordenador

**Windows:**
```powershell
# Opci√≥n 1: Descargar
# Ve a: https://ollama.ai/download

# Opci√≥n 2: Con winget
winget install Ollama.Ollama
```

### Paso 2: Descargar Modelo de C√≥digo

```bash
# Abre PowerShell o CMD y ejecuta:
ollama pull codellama

# Esto descargar√° ~4GB, puede tardar unos minutos
```

### Paso 3: Verificar que Ollama Funciona

```bash
# Probar que Ollama responde
ollama list

# Deber√≠as ver codellama en la lista
```

### Paso 4: Configurar el Proyecto

**En tu ordenador (PowerShell):**
```powershell
# Ejecuta el script de configuraci√≥n
.\configurar_ollama.ps1

# O edita .env manualmente:
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama
```

### Paso 5: Reiniciar el Servidor

```powershell
# Det√©n el servidor actual (Ctrl+C)
# Reinicia:
python start_server.py
```

### Paso 6: Probar desde Termux

```bash
cursor query "Crea una funci√≥n que sume dos n√∫meros" --write suma.py
```

## Modelos Recomendados

### Para C√≥digo (Mejor Calidad):
```bash
ollama pull codellama        # ~4GB - Especializado en c√≥digo
ollama pull deepseek-coder   # ~4GB - Muy bueno para c√≥digo
```

### Para R√°pido (Menos Calidad):
```bash
ollama pull mistral          # ~4GB - R√°pido y eficiente
ollama pull llama3.2         # ~2GB - Peque√±o y r√°pido
```

## Troubleshooting

### Ollama no responde
```bash
# Iniciar Ollama manualmente
ollama serve

# Dejar corriendo en otra terminal
```

### Modelo no encontrado
```bash
# Ver modelos instalados
ollama list

# Si no est√°, descargar:
ollama pull codellama
```

### Muy lento
- Usa un modelo m√°s peque√±o: `ollama pull mistral`
- O verifica que tengas GPU disponible

### Error de conexi√≥n
- Verifica que Ollama est√© corriendo: `ollama list`
- Verifica la URL en `.env`: `OLLAMA_BASE_URL=http://localhost:11434`

## Ventajas de Ollama

- ‚úÖ 100% Gratis
- ‚úÖ Sin l√≠mites
- ‚úÖ Privado (todo local)
- ‚úÖ Funciona offline
- ‚úÖ M√∫ltiples modelos

## Despu√©s de Configurar

Una vez configurado, puedes usar el sistema normalmente:

```bash
# Generar c√≥digo
cursor query "Crea una funci√≥n..." --write archivo.py

# Integrar c√≥digo
cursor query "A√±ade validaci√≥n" --write archivo.py

# Todo funciona igual, pero gratis!
```
