# üöÄ Gu√≠a R√°pida: Configurar Ollama (Gratis)

## Paso 1: Instalar Ollama

### Windows:
```powershell
# Opci√≥n 1: Descargar desde web
# Ve a: https://ollama.ai/download

# Opci√≥n 2: Con winget
winget install Ollama.Ollama
```

### Verificar instalaci√≥n:
```bash
ollama --version
```

## Paso 2: Descargar Modelo de C√≥digo

```bash
# Opci√≥n 1: CodeLlama (Recomendado para c√≥digo)
ollama pull codellama

# Opci√≥n 2: DeepSeek Coder (Muy bueno)
ollama pull deepseek-coder

# Opci√≥n 3: Llama 3.2 (General, r√°pido)
ollama pull llama3.2

# Opci√≥n 4: Mistral (R√°pido y eficiente)
ollama pull mistral
```

## Paso 3: Configurar en el Proyecto

```powershell
# Ejecuta el script de configuraci√≥n
.\configurar_ollama.ps1

# O edita .env manualmente:
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama
```

## Paso 4: Verificar que Ollama Est√° Corriendo

```bash
# Ollama deber√≠a iniciarse autom√°ticamente
# Verifica:
ollama list

# Si no est√° corriendo:
ollama serve
```

## Paso 5: Reiniciar el Servidor

```powershell
# Det√©n el servidor actual (Ctrl+C)
# Reinicia:
python start_server.py
```

## Paso 6: Probar desde Termux

```bash
cursor query "Crea una funci√≥n que sume dos n√∫meros" --write suma.py
```

## Troubleshooting

### Error: "No se pudo conectar con Ollama"
- Verifica que Ollama est√© corriendo: `ollama list`
- Si no est√°, ejecuta: `ollama serve`

### Error: "Model not found"
- Descarga el modelo: `ollama pull codellama`
- Verifica que est√© descargado: `ollama list`

### Muy lento
- Usa un modelo m√°s peque√±o: `ollama pull mistral` o `ollama pull llama3.2`
- O usa GPU si est√° disponible

### Modelo no genera buen c√≥digo
- Prueba con `deepseek-coder`: `ollama pull deepseek-coder`
- O `codellama`: `ollama pull codellama`

## Modelos Recomendados para C√≥digo

1. **codellama** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - Especializado en c√≥digo
2. **deepseek-coder** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - Excelente calidad
3. **llama3.2** ‚≠ê‚≠ê‚≠ê‚≠ê - Bueno y r√°pido
4. **mistral** ‚≠ê‚≠ê‚≠ê - R√°pido pero menos especializado

## Ventajas de Ollama

- ‚úÖ 100% Gratis
- ‚úÖ Sin l√≠mites
- ‚úÖ Privado (todo local)
- ‚úÖ M√∫ltiples modelos
- ‚úÖ Funciona offline

## Desventajas

- ‚ö†Ô∏è Requiere GPU o CPU potente
- ‚ö†Ô∏è Puede ser m√°s lento que APIs pagas
- ‚ö†Ô∏è Calidad depende del modelo elegido
