# üÜì Modelos Gratuitos y Sin L√≠mites

## Opci√≥n 1: Ollama (Recomendado - 100% Gratis)

**Ventajas:**
- ‚úÖ Completamente gratis
- ‚úÖ Sin l√≠mites
- ‚úÖ Funciona localmente (privado)
- ‚úÖ M√∫ltiples modelos disponibles

**Desventajas:**
- ‚ö†Ô∏è Requiere GPU o CPU potente
- ‚ö†Ô∏è Calidad puede variar seg√∫n el modelo

### Instalaci√≥n:

1. **Descarga Ollama:**
   - Windows: https://ollama.ai/download
   - O desde terminal: `winget install Ollama.Ollama`

2. **Instala un modelo de c√≥digo:**
   ```bash
   # Modelos recomendados para c√≥digo:
   ollama pull codellama        # Especializado en c√≥digo
   ollama pull deepseek-coder   # Muy bueno para c√≥digo
   ollama pull llama3.2         # Modelo general bueno
   ollama pull mistral          # R√°pido y eficiente
   ```

3. **Configura en `.env`:**
   ```bash
   AI_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=codellama
   ```

4. **Reinicia el servidor**

## Opci√≥n 2: LM Studio (Gratis, con Interfaz Gr√°fica)

**Ventajas:**
- ‚úÖ Interfaz gr√°fica amigable
- ‚úÖ F√°cil de usar
- ‚úÖ M√∫ltiples modelos

**Desventajas:**
- ‚ö†Ô∏è Requiere descargar modelos manualmente
- ‚ö†Ô∏è Requiere GPU/CPU potente

### Instalaci√≥n:

1. **Descarga LM Studio:** https://lmstudio.ai/
2. **Descarga un modelo** desde la interfaz
3. **Inicia el servidor local** en LM Studio
4. **Configura en `.env`:**
   ```bash
   AI_PROVIDER=lmstudio
   LMSTUDIO_BASE_URL=http://localhost:1234
   LMSTUDIO_MODEL=local-model
   ```

## Opci√≥n 3: Hugging Face Inference API (Gratis con l√≠mites)

Algunos modelos tienen API gratuita con l√≠mites generosos.

## Opci√≥n 4: Groq (Gratis, Muy R√°pido)

Groq ofrece API gratuita con l√≠mites generosos y es muy r√°pido.

### Configuraci√≥n Groq:

1. **Crea cuenta:** https://console.groq.com/
2. **Obt√©n API key**
3. **Configura en `.env`:**
   ```bash
   AI_PROVIDER=groq
   GROQ_API_KEY=tu-api-key
   GROQ_MODEL=llama-3.1-70b-versatile
   ```

## üéØ Recomendaci√≥n: Ollama con CodeLlama

Para desarrollo de c√≥digo, **Ollama + CodeLlama** es la mejor opci√≥n gratuita:

```bash
# Instalar Ollama
# Luego:
ollama pull codellama

# Configurar .env:
AI_PROVIDER=ollama
OLLAMA_MODEL=codellama
```

## üìä Comparaci√≥n de Modelos Gratuitos

| Modelo | Calidad C√≥digo | Velocidad | Requisitos |
|--------|---------------|-----------|------------|
| CodeLlama (Ollama) | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | GPU recomendada |
| DeepSeek Coder | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | GPU recomendada |
| Llama 3.2 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | CPU suficiente |
| Mistral | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | CPU suficiente |

## üöÄ Setup R√°pido con Ollama

```powershell
# 1. Instalar Ollama (si no est√°)
winget install Ollama.Ollama

# 2. Reiniciar terminal, luego:
ollama pull codellama

# 3. Configurar .env
.\configurar_ollama.ps1

# 4. Reiniciar servidor
python start_server.py
```
